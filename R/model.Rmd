---
title: "`r params$title`"
author: "Ryan Heslin"
date: "`r format(Sys.Date(), '%B %e, %Y')`"
header-includes:
  - \setlength{\parindent}{2em}
  - \setlength{\parskip}{2em}
params:
    title: "Predicting Chocolate Bar Ratings"
output:
  pdf_document:
    highlight: "kate"
    df_print: "kable"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  comment = "",
  fig.pos = "",
  message = FALSE,
  tidy = "styler",
  warning = FALSE,
  fig.align = "center",
  highlight = TRUE
)
```

# Introduction 

This project's goal is to predict the ratings assigned by taste-testers to each of several thousand chocolate bars. 
Ratings are on the interval $[1, 4]$, in increments of 0.25, with higher ratings being better. 
Available features include bar ingredients, 
company and country of origin, cocoa percentage, and words used by testers to describe the bar.

The data come from a recent Tidy Tuesday and can be found [here](https://github.com/rfordatascience/tidytuesday/blob/master/data/2022/2022-01-18/readme.md).

Available features are:




As a first pass, I fit a simple linear regression predicting ratings. I don't expect it to do well, but it will provide a useful baseline.
I create a "sentiment" feature by using `sentimentr`
to compute sentiment for each phrase used to describe each chocolate, then summing the 
total for each chocolate bar. Positive values 
mean higher sentiment. I don't simply compute 
sentiments for individual words because that 
would miscode negations (e.g., "not good").


It might have been better to extract this complex transformation to a function instead of jamming it into a recipe.


I also define my own recipe step, following the 
tutorial [here](https://www.tidymodels.org/learn/develop/recipes/). It takes a number of character vectors and returns dummy columns representing whether each of the $n$ most 
common elements across all the columns is present in each row. 
```{r}
suppressMessages(library(tidymodels))
library(tidyverse)
source(here::here("R", "utils.R"))
set.seed(12345)
```

I noted that words used to describe chocolate bars 
were often ingredients or descriptions of taste. 
These might be useful categorical features.
```{r}
select(train, starts_with("characteristic")) |>
  unlist(use.names = FALSE) |>
  table_head(.n = 6) |>
  vec2DF()
```

```{r}
chocolate_raw <- readRDS(here::here("data", "chocolate.Rds"))
chocolate_raw[["sentiment"]] <- NA
chocolate <- initial_split(chocolate_raw)
train <- training(chocolate)
test <- testing(chocolate)
lin_spec <- linear_reg()
lin_rec <- recipe(train,
  formula = rating ~ .,
  vars = NULL
) |>
  step_mutate(
    sentiment = (across(starts_with("characteristic"), ~ str_remove_all(.x, "\\.") |>
      sentimentr::get_sentences() |>
      sentimentr::sentiment() |>
      pull("sentiment")) |>
      rowSums() |>
      unname()),
    across(where(is.logical), as.integer),
  ) |>
  step_term_dummy(starts_with("characteristic"), n = 10, match_terms = NULL, role = "predictor") |>
  step_other(company_location, country_of_bean_origin) |>
  step_dummy(c(all_nominal_predictors())) |>
  step_novel(all_nominal_predictors()) |>
  update_role(rating, new_role = "outcome") |>
  update_role(company_location, country_of_bean_origin, review_date, cocoa_percent, cocoa_butter, beans, lecithin, sugar, salt, nonsugar_sweetener, vanilla, sentiment, new_role = "predictor") |>
  update_role(ref, company_manufacturer, ingredients, specific_bean_origin_or_bar_name,
    starts_with("characteristic"),
    new_role = "id"
  )


lin_wflow <- workflow(lin_rec, lin_spec)
```

I define a function (in `utils.R`) to round model predictions to the nearest increment on the scale. This violates 
the OLS assumptions, but this model is only intended as a reference. I use this function to develop a custom metric, `interval_error`, that measures 
each raw prediction's deviation from the true rating, rounded to the nearest increment, in units of the increment (0.25). For example, 
a prediction of 3.27 and a true rating of 2.5 calibrate to an interval error of 3 because 
$|2.5-3.27|$ rounded to the nearest fourth and multiplied by 4 (the inverse of the increment) is 3. Estimates below the scale's minimum are rounded up to it, and those above the maximum are rounded down to it. Stated mathematically, the average interval error is:

\[
   \text{AIE} = \frac{1}{n} \sum_{i=1}^n \min \left \{ \max \left [ \left |\frac{1}{\text{interval}}\left (y_i - \left \lfloor \hat y_i + \frac{1}{2} \right \rfloor \right ) \right |, \min(y) \right ], \max(y) \right \}
\]
where $\lfloor \, \rfloor$ designates the floor function.
(See [here](https://math.stackexchange.com/questions/681252/notation-for-rounding-in-equation) for an explanation of the formula).

Using this approach, mean absolute error is below one and a half increments - better than I expected. RMSE is somewhat higher, suggesting some errors are disproportionately large. 
The average absolute error is equal to about 1.3 intervals.
```{r}
chocolate_metrics <- metric_set(mae, rmse, interval_error)
baked <- prep(lin_rec) |>
  bake(train)
lin_model <- fit(lin_wflow, train)
tidy(lin_model)
lin_preds <- augment(lin_model, new_data = train)
lin_preds[[".pred"]] <- fit_to_scale(lin_preds[[".pred"]], min = 1, max = 4, increment = .25)
chocolate_metrics(lin_preds, truth = rating, estimate = .pred)
```

It seems the biggest errors are for the very worst 
chocolate.
```{r}
range(lin_preds[[".pred"]])
arrange(lin_preds, -abs(.pred - rating)) |>
  head() |>
  select(company_manufacturer, specific_bean_origin_or_bar_name, .pred, rating)
```


Next, I try a random forest, reasoning that the relatively large number of predictors and likelihood 
of nonlinear associations suit it well for these data.
```{r}
baked <- prep(lin_rec) |> bake(train)
combined <-
  left_join(baked, select(
    train,
    all_of(setdiff(union(colnames(train), lin_rec[[c("var_info", "variable")]]), colnames(baked))), ref
  ),
  by = "ref"
  )
folds <- vfold_cv(combined, v = 20)

rf_spec <- rand_forest(mtry = tune(), trees = 400, min_n = tune()) |>
  set_engine(engine = "ranger", importance = "permutation") |>
  set_mode("regression")
rf_wf <- workflow(lin_rec, rf_spec)
```

I use 20-fold cross validation to select the `mtry` and `min_n` parameters.
```{r}
mtry_start <- floor(nrow(tidy(lin_model)) / 3)
tuning_grid <- grid_regular(mtry(range = c(mtry_start - 2, mtry_start + 4)), min_n(range = c(5, 40)))
tune_res <- tune_grid(rf_wf,
  grid = tuning_grid, resamples = folds,
  metrics = metric_set(interval_error)
)

rf_wf <- finalize_workflow(rf_wf, select_best(tune_res, "interval_error"))
rf_model <- fit(rf_wf, data = train)
rf_model
rf_preds <- predict(rf_model, new_data = train)
```

These predictions have a much higher range. MAE and 
RMSE are less than half what they were in regression
```{r}
rf_preds <- augment(rf_model, new_data = train)
rf_preds[[".pred"]] <- fit_to_scale(rf_preds[[".pred"]], min = 1, max = 4, increment = .25)
chocolate_metrics(rf_preds, truth = rating, estimate = .pred)
```

Sentiment is by far the most important variable, 
followed by some of the ingredient dummies. 
The country of origin dummies don't seem that 
important.
```{r}
rf_model |>
  extract_fit_engine() |>
  importance() |>
  enframe(name = "variable", value = "importance") |>
  mutate(variable = factor(variable, levels = variable[order(importance, decreasing = TRUE)])) |>
  ggplot(aes(x = variable, y = importance)) +
  geom_col(aes(fill = variable), show.legend = FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

I'm concerned about overfitting, so I also try boosting, which is less prone to overfitting.
```{r}
boost_spec <- boost_tree(
  mode = "regression", learn_rate = tune(),
  mtry = tune(),
  min_n = tune(), tree_depth = 15, sample_size = 1
)
```


```{r}
boost_wf <- workflow(lin_rec, boost_spec)
tuning_grid <- grid_regular(
  mtry(range = c(mtry_start - 2, mtry_start + 4)),
  learn_rate(),
  min_n()
)
tune_res <- tune_grid(boost_wf, grid = tuning_grid, resamples = folds, metrics = metric_set(interval_error))
boost_wf <- finalize_workflow(boost_wf, select_best(tune_res, "interval_error"))
boost_model <- fit(boost_wf, data = train)
boost_model
boost_preds <- predict(boost_model, new_data = train)
```

The boosted model does terribly compared to the others, with far greater errors.
```{r}
boost_preds <- augment(boost_model, new_data = train)
boost_preds[[".pred"]] <- fit_to_scale(boost_preds[[".pred"]], min = 1, max = 4, increment = .25)
chocolate_metrics(boost_preds, truth = rating, estimate = .pred)
```


# Comparison

This plot clarifies the pattern
```{r}
overall <- tibble(
  actual = train[["rating"]], linear = lin_preds[[".pred"]],
  rf = rf_preds[[".pred"]], boosted = boost_preds[[".pred"]]
)
fitted_actual_plot(overall, -actual)
```

# Evaluation

Now, to predict on the test set. The boosted model 
does as terribly as before. Linear regression performs about the same as the random forest. 
The random forest's better performance on the 
training set was probably a result of overfitting.
```{r}
lin_test <- test_preds(lin_model, test)
rf_test <- test_preds(rf_model, test)
boost_test <- test_preds(boost_model, test)

bind_rows(
  linear = chocolate_metrics(lin_test, estimate = .pred, truth = rating),
  rf = chocolate_metrics(rf_test, estimate = .pred, truth = rating),
  boost = chocolate_metrics(boost_test, estimate = .pred, truth = rating),
  .id = "model"
)
```

The same patterns appear when plotting the test predictions. The boosted model seems to be biased low. The random forest predicts in a wider range than the linear model, but has 
comparable performance by overall error metrics. 
I would prefer the linear model here because it 
is much simpler and easier to interpret.
```{r}
test_overall <- tibble(
  actual = test[["rating"]], linear = lin_test[[".pred"]],
  rf = rf_test[[".pred"]], boosted = boost_test[[".pred"]],
  ref = test[["ref"]]
)
fitted_actual_plot(overall, -actual)
```

Again, the low-rated bars confounded the models the most.
```{r}
left_join(test_overall, test, by = "ref") |>
  select(specific_bean_origin_or_bar_name, actual, linear, rf, boosted) |>
  arrange(-abs(linear - actual)) |>
  head()
```

# Assessment 

I enjoyed working on this project. I think I used an effective EDA strategy that identified ways the 
ingredient and characteristic variables could be 
used. Constructing features based on sentiment took some work, but was well worth doing. I also took the opportunity to delve into the 
`tidymodels` interface by defining a custom metric and recipe step. I probably could have gotten 
by using built-in tools, but I enjoyed customizing them. 

I think I caused myself problems by storing the 
words used to describe each chocolate bar in 
five atomic vector columns. A single list column 
would have made more conceptual sense and required less work to transform for analysis. I 
also could have done more to fine-tune the random 
forest model, or perhaps experimented with 
KNN after using the lasso or PCA to reduce the 
number of variables. 

